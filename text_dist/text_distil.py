# -*- coding: utf-8 -*-
"""Copy of Text_Distillation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PP-WznXPUrF96_NBfC0lNqY5A1-G0kn8
"""
"""
#clone the repo
!git clone https://github.com/ErikEkstedt/TurnGPT.git


!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd /kaggle/working/TurnGPT
# for google Colab always replace /kaggle/working with /content

!ls

# This will install PyTorch, PyTorch Lightning, Transformers
!pip install -r requirements.txt

#install turngpt model
!pip install -e .

#!pip install datasets
"""

import torch
from turngpt.model import TurnGPT
import torch.nn.functional as F
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from datasets import load_dataset
#from torch.utils.data import DataLoader

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# !rm -rf .git

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
torch.cuda.empty_cache()

"""# Train Turngpt"""

#!ls /kaggle/working/TurnGPT/turngpt
#when using google colab replace only /kaggle/working with /content

"""# Initialise turngpt"""

import torch
from turngpt.tokenizer import SpokenDialogTokenizer
from turngpt.projection_labeler import ProjectionLabeler
#from your_turngpt_module import TurnGPT  # adjust import as needed

#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1.1) Initialize TurnGPT with a TRP projection head
teacher = TurnGPT(
    pretrained_model_name_or_path="gpt2-xl",  # deepest model
    pretrained=True,                       # load GPT-2 pretrained weights
    trp_projection_steps=1,                # any positive integer ▸ build TRP head
    trp_projection_type="linear",          # default linear projection
    omit_dialog_states=False,              # keep speaker_ids in input
    no_train_first_n=0,                    # don’t mask out initial tokens
    learning_rate=1e-4,                    # only for hparams; unused at infer
    weight_loss=False,
    weight_regular_token=0.5,
    weight_eos_token=1.0,
)

# initilaises the specialtoken tokenizer
teacher.init_tokenizer()

# 1.3) Initialize the <ts> embedding
teacher.initialize_special_embeddings()

# 1.4) Move to GPU and set eval mode
teacher.to(device)

# You can also print the total number of parameters
total_params = sum(p.numel() for p in teacher.parameters())
print(f"\nTotal number of parameters in teacher model: {total_params}")

#

"""# Preprocessing the datasets(Used dialog datasets)"""

from turngpt.projection_labeler import ProjectionLabeler
from torch.utils.data import DataLoader, Dataset
import logging

from turngpt.projection_labeler import ProjectionLabeler
from torch.utils.data import DataLoader, Dataset
import logging
import torch # Import torch to use unsqueeze and squeeze

class DailyDialogDataset(Dataset):
    def __init__(self, dataframe,
                 teacher_tokenizer, projection_steps, eos_token_id,
                 max_length=128):
        self.dialogs = dataframe["dialog"].tolist()
        #self.student_tokenizer = student_tokenizer
        self.teacher_tokenizer = teacher_tokenizer
        self.max_length = max_length

        # 3.1) Build ProjectionLabeler (it will run on teacher’s token IDs)
        self.labeler = ProjectionLabeler(
            projection_steps=projection_steps,
            token_id=eos_token_id
        )

    # Add the __len__ method
    def __len__(self):
        return len(self.dialogs)

    def __getitem__(self, idx):
        turns = self.dialogs[idx]
        # --- same “strip A:/B:”, prepend [SPEAKER_A]/[SPEAKER_B], join <ts>
        processed_turns = []
        speakers = ["[SPEAKER_A]", "[SPEAKER_B]"]
        for i, turn in enumerate(turns):
            utterance = turn.split(":", 1)[-1].strip() if ":" in turn else turn.strip()
            processed_turns.append(f"{speakers[i % 2]} {utterance}")
        text = "<ts>".join(processed_turns).strip() + "<ts>"


        # 3.3) Teacher tokenization (SpokenDialogTokenizer knows <speaker1>/<speaker2>, <ts>)
        teacher_enc = self.teacher_tokenizer(
            text,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=self.max_length
        )

        # 3.4) Compute binary TRP labels from teacher input_ids:
        #      labeler outputs 0/1 for each position [S]
        input_ids_for_label = teacher_enc["input_ids"]#.squeeze(0)  # [S]


        # This might be necessary if ProjectionLabeler expects batched input
        #input_ids_for_label = input_ids_for_label.unsqueeze(0) # [1, S]

        # Call the ProjectionLabeler with the now batched input
        trp_labels = self.labeler(input_ids_for_label)           # Should receive [1, S]

        # --- WORKAROUND: Squeeze batch dimension back as dataset should return [S] ---
        # Assuming ProjectionLabeler returns a batched output [1, S] for [1, S] input
        trp_labels = trp_labels.squeeze(0) # [S], ints 0 or 1


        return {

            "attention_mask":teacher_enc["attention_mask"].squeeze(0),
            "speaker_ids":    teacher_enc["speaker_ids"].squeeze(0),
            "input_ids":teacher_enc["input_ids"].squeeze(0),
            "trp_labels":     trp_labels,  # [S]
            "text":           text          # string
        }

import pandas as pd
#load data
splits = {
    'train': 'data/train-00000-of-00001-f151c79abb2c1fd5.parquet',
    'validation': 'data/validation-00000-of-00001-2407eb323af19881.parquet',
    'test': 'data/test-00000-of-00001-66dc7d981b70c918.parquet'
}
df_train = pd.read_parquet("hf://datasets/OpenRL/daily_dialog/" + splits["train"])
df_val = pd.read_parquet("hf://datasets/OpenRL/daily_dialog/" + splits["validation"])
df_test=pd.read_parquet("hf://datasets/OpenRL/daily_dialog/" + splits["test"])

#creating train, val and test loaders
train_dataset = DailyDialogDataset(
    dataframe=df_train,
    #student_tokenizer=student_tokenizer,
    teacher_tokenizer=teacher.tokenizer,         # from teacher.init_tokenizer()
    projection_steps=teacher.trp_projection_steps,
    eos_token_id=teacher.tokenizer.eos_token_id, # <ts> ID
    max_length=128
)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=3)

val_dataset = DailyDialogDataset(
    dataframe=df_val,
    #student_tokenizer=student_tokenizer,
    teacher_tokenizer=teacher.tokenizer,
    projection_steps=teacher.trp_projection_steps,
    eos_token_id=teacher.tokenizer.eos_token_id,
    max_length=128
)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=3)


test_dataset = DailyDialogDataset(
    dataframe=df_test,
    #student_tokenizer=student_tokenizer,
    teacher_tokenizer=teacher.tokenizer,
    projection_steps=teacher.trp_projection_steps,
    eos_token_id=teacher.tokenizer.eos_token_id,
    max_length=128
)
test_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=3)

sample=next(iter(train_loader))
print(sample['input_ids'][0])
print(sample['trp_labels'][0])

"""# Training of Turngpt as a teacher model"""

from pytorch_lightning.callbacks import ModelCheckpoint

checkpoint_callback = ModelCheckpoint(
    monitor="val_loss",        # Name of the metric to monitor
    dirpath="checkpoints/turngpt",    # Where to save the checkpoints
    filename="turngpt-{epoch:02d}-{val_loss:.2f}",  # File naming convention
    save_top_k=1,              # Save only the best model (or change as needed)
    mode="min",                 # Lower validation loss is considered better

)

import pytorch_lightning as pl

trainer = pl.Trainer(
    max_epochs=5,
    accelerator="gpu",
    devices=1,
    callbacks=[checkpoint_callback],
    val_check_interval=1.0,      # Run validation at the end of each epoch, or use a fraction (e.g., 0.5)
    default_root_dir="logs/"     # Optional: where logs and checkpoints will be stored
)

trainer.fit(teacher, train_dataloaders=train_loader, val_dataloaders=val_loader)

"""# load the model from checkpoint"""

#!pwd

# Commented out IPython magic to ensure Python compatibility.
#check checkpoint dir
# %cd /kaggle/working/TurnGPT/checkpoints/turngpt

#get the saved checkpoint

#!ls

#download the checkpoint
#!zip -r checkpoint.zip turngpt-epoch=01-val_loss=1.50.ckpt
#!zip -r /kaggle/working/checkpoint.zip turngpt-epoch=01-val_loss=1.50.ckpt

import os
print(os.listdir('.'))

#load the trained teacher model
teacher_model=TurnGPT.load_from_checkpoint("turngpt-epoch=01-val_loss=1.50.ckpt")

#evaluate model
teacher_model.to(device).eval()

"""# Evaluate model predictions on some text"""

# Define the corrected idx_to_string method
def corrected_idx_to_string(self, idx):
    if isinstance(idx, torch.Tensor):
        idx = idx.item()
    s = self.tokenizer.convert_ids_to_tokens(idx)
    # Pass a list containing the single stripped token string
    s = self.tokenizer.convert_tokens_to_string([s.strip()])
    return s

# Replace the original idx_to_string method with the corrected one
teacher_model.idx_to_string = corrected_idx_to_string.__get__(teacher_model, teacher_model.__class__)

print("Corrected idx_to_string method applied to teacher_model.")

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

example=next(iter(test_loader))
text=example['text'][0]
text

# Get trp from a list of text lists
out = teacher_model.string_list_to_trp(text)

  # out: dict_keys(['logits', 'past_key_values', 'probs', 'trp_probs', 'tokens'])

  # Simple Plot
import matplotlib.pyplot as plt
#%matplotlib inline

import matplotlib.pyplot as plt
import torch

def plot_trp(P, text, figsize=(20, 4), rotation=45):
    # Create a larger figure for better readability
    fig, ax = plt.subplots(1, 1, figsize=figsize)

    # Convert tensor to numpy array
    x = torch.arange(len(P)).cpu().numpy()
    ax.bar(x, P.cpu().numpy(), color="skyblue")

    # Set x-axis ticks and labels
    ax.set_xticks(x)
    ax.set_xticklabels(text, rotation=rotation, fontsize=10)

    # Adjust spacing on the x-axis to avoid crowding
    plt.tight_layout()

    # Set y-axis limits if needed
    ax.set_ylim([0, 1])

    plt.pause(0.01)
    return fig, ax

# Example usage:
fig, ax = plot_trp(out["trp_probs"][0], out["tokens"][0])
plt.show()

"""# Create the student model"""

import torch
import torch.nn as nn
import pytorch_lightning as pl
import torch.nn.functional as F
from transformers import AutoModelForCausalLM
from turngpt.tokenizer import SpokenDialogTokenizer
from turngpt.projection_labeler import ProjectionLabeler

#the student model uses the specialtokenizer of the teacher
class StudentWithTRP(pl.LightningModule):
    def __init__(
        self,
        student_model_name="distilgpt2",
        trp_projection_steps=1,
        learning_rate=5e-5,
    ):
        super().__init__()
        self.save_hyperparameters()

        # === Tokenizer ===
        self.tokenizer = SpokenDialogTokenizer(pretrained_model_name_or_path=student_model_name)

        # === Backbone ===
        self.backbone = AutoModelForCausalLM.from_pretrained(student_model_name)
        self.backbone.resize_token_embeddings(len(self.tokenizer))
        self.initialize_special_embeddings()

        # === TRP Head ===
        hidden_size = self.backbone.config.hidden_size
        self.trp_projection_steps = trp_projection_steps
        self.trp_projection_head = nn.Linear(hidden_size, 1)

        if trp_projection_steps > 0:
            self.labeler = ProjectionLabeler(
                projection_steps=trp_projection_steps,
                token_id=self.tokenizer.eos_token_id,
            )

    def initialize_special_embeddings(self, tokens=["!", "?", "."]):
        ts_id = self.tokenizer.eos_token_id
        with torch.no_grad():
            ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens)).to(self.backbone.device)
            avg_emb = self.backbone.transformer.wte(ids).mean(0)
            self.backbone.transformer.wte.weight.data[ts_id] = avg_emb
        print(f"Initialized {self.tokenizer.eos_token} -> avg({tokens})")

    def forward(self, input_ids, attention_mask=None, speaker_ids=None):
        outputs = self.backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=speaker_ids,
            output_hidden_states=True,
            return_dict=True,
        )
        lm_logits = outputs.logits  # [B, S, V]
        hidden_states = outputs.hidden_states[-1]  # [B, S, H]
        trp_logits = self.trp_projection_head(hidden_states).squeeze(-1)  # [B, S]
        return {"lm_logits": lm_logits, "trp_logits": trp_logits}

    def configure_optimizers(self):
        return torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)

#init student model
student_model = StudentWithTRP(
    student_model_name="distilgpt2",
    trp_projection_steps=1  # or match your setup
)

student_model.to(device)

total_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)
print(f"Trainable parameters: {total_params:,}")

#test the student model on some text
#student model also uses turngpt's tokenizer
text="Hey, how are you doing!"
enc=student_model.tokenizer(text)
enc

"""# Knowledge Distillation Training

Here training is done for two losses,LM teacher and student logits losses and binary cross entropy loss between the teacher's trp probabilities and student logits

"""

import torch
import torch.nn.functional as F
from torch.optim import AdamW
from tqdm.notebook import tqdm  # Notebook-friendly tqdm
from IPython.display import clear_output

# Hyperparameters
#more weight on TRP BCE loss
num_epochs = 10
T = 3.0               # Temperature for distillation
alpha_trp = 3.0       # Weight for TRP (BCE) loss
alpha_lm = 1.0        # Weight for LM (KL) loss
lr = 5e-5

optimizer = AdamW(student_model.parameters(), lr=lr)

for epoch in range(1, num_epochs + 1):
    # Clear previous output to update the progress bar on one line
    clear_output(wait=True)
    student_model.train()
    epoch_loss = 0.0

    print(f"\n--- Epoch {epoch}/{num_epochs} ---")


    pbar = tqdm(train_loader, desc="Training batches", leave=False, dynamic_ncols=True)
    for batch in pbar:
        # Move inputs to device
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        speaker_ids = batch["speaker_ids"].to(device)

        # Teacher forward pass with no gradient updates
        with torch.no_grad():
            teach_out = teacher_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                speaker_ids=speaker_ids,
                return_dict=True
            )
            # Get teacher TRP logits and LM logits
            teacher_trp_logits = teach_out.mc_logits   # Shape: [B, S]
            teacher_lm_logits = teach_out.logits        # Shape: [B, S, V]

        # Student forward pass
        student_out = student_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            speaker_ids=speaker_ids
        )
        # Get student TRP and LM logits
        student_trp_logits = student_out["trp_logits"]   # Shape: [B, S]
        student_lm_logits = student_out["lm_logits"]       # Shape: [B, S, V]

        # ---------------------
        # TRP Loss (BCE with soft targets)
        # ---------------------
        # Remove the last time step for alignment
        teacher_trp_probs = torch.sigmoid(teacher_trp_logits[:, :-1])  # Shape: [B, S-1]
        student_trp_logits = student_trp_logits[:, :-1]                # Shape: [B, S-1]
        trp_mask = attention_mask[:, 1:].float()  # Mask for valid positions

        # Compute masked binary cross entropy loss
        bce_loss = F.binary_cross_entropy_with_logits(
            student_trp_logits, teacher_trp_probs, reduction="none"
        )
        bce_loss = (bce_loss * trp_mask).sum() / trp_mask.sum()

        # ---------------------
        # LM Loss (KL divergence with masking)
        # ---------------------
        # Compute KL divergence per position
        kl_per_position = F.kl_div(
            F.log_softmax(student_lm_logits[:, :-1] / T, dim=-1),
            F.softmax(teacher_lm_logits[:, :-1] / T, dim=-1),
            reduction="none"
        ).sum(-1)  # Shape: [B, S-1]

        # Mask for valid positions (shifted attention mask)
        lm_mask = attention_mask[:, 1:].float()  # Shape: [B, S-1]

        # Compute masked KL loss
        kl_lm_loss = (kl_per_position * lm_mask).sum() / lm_mask.sum() * (T ** 2)

        # ---------------------
        # Combine losses
        total_loss = alpha_trp * bce_loss + alpha_lm * kl_lm_loss

        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        epoch_loss += total_loss.item()

        # Update tqdm progress bar with current loss values
        pbar.set_postfix({
            "TRP_BCE": f"{bce_loss.item():.4f}",
            "LM_KL": f"{kl_lm_loss.item():.4f}",
            "Total": f"{total_loss.item():.4f}"
        })

    print(f"Epoch {epoch} | Loss: {epoch_loss / len(train_loader):.4f}")

"""# Save the student model and load for evaluation"""

# Save the entire state dictionary.
torch.save(student_model.state_dict(), "student_with_trp.pt")

#save student state dictionary as zip

#!zip -r /kaggle/working/student_state_dict.zip  student_with_trp.pt
#download the checkpoint
#!zip -r checkpoint.zip turngpt-epoch=01-val_loss=1.50.ckpt
#!zip -r /kaggle/working/checkpoint.zip turngpt-epoch=01-val_loss=1.50.ckpt

#download zipfile
#from IPython.display import FileLink
#display(FileLink('checkpoint.zip'))
#display(FileLink("/kaggle/working/student_state_dict.zip"))

"""# Distilled student model"""

# load the saved model ---
distil_student_model = StudentWithTRP(student_model_name="distilgpt2")
distil_student_model.backbone.resize_token_embeddings(len(distil_student_model.tokenizer))  # ensure matching vocab size
state_dict = torch.load("student_with_trp.pt", map_location=torch.device("cpu"))
distil_student_model.load_state_dict(state_dict)
distil_student_model.to(device).eval()

total_params = sum(p.numel() for p in distil_student_model.parameters() if p.requires_grad)
print(f"Trainable parameters: {total_params:,}")

total_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)
print(f"Trainable parameters: {total_params:,}")

test_example=next(iter(test_loader))
text_1=test_example['text'][0]
text_1

"""# Test the distilled student model version on some sample text"""

import torch
import matplotlib.pyplot as plt

sample_text = (
    "[SPEAKER_A] Good morning , sir . Is there a bank near here ?<ts>"
    "[SPEAKER_B] There is one . 5 blocks away from here ?<ts>"
    "[SPEAKER_A] Well , that's too far.Can you change some money for me ?<ts>"
    "[SPEAKER_B] Surely , of course . What kind of currency have you got ?<ts>"
    "[SPEAKER_A] RIB .<ts>"
    "[SPEAKER_B] How much would you like to change ?<ts>"
    "[SPEAKER_A] 1000 Yuan.Here you are .<ts>"
)

# Tokenize the sentence (with padding and truncation, if applicable)
encoding = distil_student_model.tokenizer(
    sample_text,
    return_tensors="pt",
    padding=True,
    truncation=True
)
input_ids = encoding["input_ids"].to(distil_student_model.device)        # Shape: [1, S]
attention_mask = encoding["attention_mask"].to(distil_student_model.device)
speaker_ids=encoding["speaker_ids"].to(distil_student_model.device)


# -------------------------
distil_student_model.eval()  # Ensure the model is in evaluation mode
with torch.no_grad():
    outputs = distil_student_model(input_ids=input_ids, attention_mask=attention_mask,speaker_ids=speaker_ids)
trp_logits = outputs["trp_logits"]  # Shape: [1, S]

# Convert logits to probabilities
trp_probs = torch.sigmoid(trp_logits)

#
post_eos = torch.zeros((trp_probs.size(0), 1), device=trp_probs.device)
trp_probs = torch.cat([trp_probs, post_eos], dim=1)

# -------------------------
# Step 3: Decode the Input IDs into a Human‑Readable Text
# -------------------------
full_decoded_text =distil_student_model.tokenizer.decode(input_ids[0])
print("Decoded Text:")
print(full_decoded_text)


# Step 4: Plot the TRP Probabilities with the Full Decoded Text as Title
# -------------------------
def plot_trp_with_decoded_title(P, input_ids, distil_student_model, figsize=(40, 6), rotation=90, label_every=1, fontsize=10):
    """
    Creates a bar plot for TRP probabilities and uses the full decoded text as the title.

    Parameters:
      P         : A tensor of TRP probabilities for a single sample (shape [S]).
      input_ids : A tensor of token IDs used for decoding (first example).
      model     : Your model instance (to access its tokenizer).
      figsize   : Figure size for the plot.
      rotation  : Rotation of x-axis labels.
      label_every: Display x-axis label every 'label_every' tokens.
      fontsize  : Font size for the labels.
    """
    # Decode full text for title
    full_decoded_text = distil_student_model.tokenizer.decode(input_ids[0])


    # Create the bar plot
    fig, ax = plt.subplots(figsize=figsize)
    x_coords = torch.arange(len(P)).cpu().numpy()
    ax.bar(x_coords, P.cpu().numpy(), color="skyblue")

    # Prepare subword token labels for the x-axis and "clean" them (e.g., replace the Ġ markers)
    raw_tokens = distil_student_model.tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    cleaned_tokens = [token.replace("Ġ", " ") for token in raw_tokens]

    # Label every "label_every" token to avoid clutter
    indices = list(range(0, len(cleaned_tokens), label_every))
    selected_tokens = [cleaned_tokens[i] for i in indices]
    ax.set_xticks(indices)
    ax.set_xticklabels(selected_tokens, rotation=rotation, fontsize=fontsize)
    ax.set_ylim([0, 1])

    # Set the full decoded text as the plot title
    ax.set_title(full_decoded_text, fontsize=fontsize+2)

    plt.tight_layout()
    plt.savefig("/kaggle/working/sample_text_plot.png" )
    plt.show()
    return fig, ax

# Plot the TRP probabilities for the first (and only) sample.
plot_trp_with_decoded_title(trp_probs[0], input_ids, distil_student_model)

"""# Plotting Full Token Turn Prediction on the Test Data Using Distilled Student model"""

import os
import torch
import matplotlib.pyplot as plt
import zipfile
import shutil

# Create directory to save plots
save_dir = "llm_project"
os.makedirs(save_dir, exist_ok=True)

# Keep track of saved plot files
saved_files = []

def plot_trp_full_tokens(P, input_ids, model, figsize=(40, 6), rotation=90, label_every=1, fontsize=10):
    raw_tokens = model.tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    cleaned_tokens = [token.replace("Ġ", " ") for token in raw_tokens]
    full_decoded_text = model.tokenizer.decode(input_ids[0],skip_special_tokens=True)

    fig, ax = plt.subplots(figsize=figsize)
    x_coords = list(range(len(P)))
    ax.bar(x_coords, P.detach().cpu().numpy(), color="skyblue", edgecolor="black")

    indices = list(range(0, len(cleaned_tokens), label_every))
    selected_tokens = [cleaned_tokens[i] for i in indices]
    ax.set_xticks(indices)
    ax.set_xticklabels(selected_tokens, rotation=rotation, fontsize=fontsize)

    ax.set_xlabel("Tokens", fontsize=fontsize + 2)
    ax.set_ylabel("TRP Probability", fontsize=fontsize + 2)
    ax.set_title(full_decoded_text, fontsize=fontsize + 4, pad=20)
    ax.set_ylim([0, 1])
    ax.grid(True, axis="y", linestyle="--", alpha=0.6)

    plt.tight_layout()

    # Save figure
    filename = f"{save_dir}/plot_{len(saved_files)}.png"
    plt.savefig(filename)
    saved_files.append(filename)
    plt.close(fig)

    return fig, ax


# -------- Run model and generate plots --------
for batch in test_loader:
    device = next(distil_student_model.parameters()).device
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    speaker_ids = batch["speaker_ids"].to(device)

    distil_student_model.eval()
    with torch.no_grad():
        outputs = distil_student_model(input_ids=input_ids, attention_mask=attention_mask, speaker_ids=speaker_ids)
    trp_logits = outputs["trp_logits"]
    trp_probs = torch.sigmoid(trp_logits)
    post_eos = torch.zeros((trp_probs.size(0), 1), device=trp_probs.device)
    trp_probs = torch.cat([trp_probs, post_eos], dim=1)

    for i in range(trp_probs.size(0)):
        sample_probs = trp_probs[i]
        sample_input_ids = input_ids[i].unsqueeze(0)
        plot_trp_full_tokens(sample_probs, sample_input_ids, distil_student_model)

    break  # Only run on the first batch for now

# -------- Create a zip of saved plots --------
zip_filename = "llm_project_plots_1.zip"
zip_path = os.path.join("/kaggle/working", zip_filename)

with zipfile.ZipFile(zip_path, 'w') as zipf:
    for filepath in saved_files:
        if os.path.isfile(filepath):
            zipf.write(filepath, arcname=os.path.basename(filepath))

print(f"[✅] Zip file created at: {zip_path}")

# -------- List files to confirm --------
print("\n📁 Files in /kaggle/working/:")
for f in os.listdir("/kaggle/working"):
    print(" -", f)

text_2=test_example["input_ids"][0]
print(text_2)

out=distil_student_model(text_2.to(device))
probs=out["trp_logits"]

turn=torch.sigmoid(probs)
#turn
preds = (turn  > 0.22).long().squeeze(0).cpu().numpy()
print(preds)

labels=test_example["trp_labels"][0]
x=labels.squeeze(0).cpu().numpy()
print(x)

import torch
from sklearn.metrics import precision_recall_curve, auc
from tqdm import tqdm

def compute_pr_auc(y_true, y_scores):
    precision, recall, _ = precision_recall_curve(y_true, y_scores)
    pr_auc_score = auc(recall, precision)
    return pr_auc_score

def evaluate_event_based(y_true, y_probs, window=3, threshold=0.5):
    hits = 0
    misses = 0
    false_alarms = 0
    used_pred_indices = set()

    gt_indices = [i for i, val in enumerate(y_true) if val == 1]
    pred_indices = [i for i, val in enumerate(y_probs) if val >= threshold]

    for gt in gt_indices:
        matched = False
        for pred in pred_indices:
            if abs(pred - gt) <= window and pred not in used_pred_indices:
                hits += 1
                used_pred_indices.add(pred)
                matched = True
                break
        if not matched:
            misses += 1

    false_alarms = len(pred_indices) - hits

    precision = hits / (hits + false_alarms + 1e-8)
    recall = hits / (hits + misses + 1e-8)
    f1 = 2 * precision * recall / (precision + recall + 1e-8)

    return {
        'Event Precision': precision,
        'Event Recall': recall,
        'Event F1': f1,
        'Hits': hits,
        'Misses': misses,
        'False Alarms': false_alarms
    }

# === Evaluation loop ===

all_probs = []
all_labels = []

device = next(distil_student_model.parameters()).device

distil_student_model.eval()

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Evaluating"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        speaker_ids = batch.get("speaker_ids")
        if speaker_ids is not None:
            speaker_ids = speaker_ids.to(device)

        outputs = distil_student_model(input_ids=input_ids, attention_mask=attention_mask, speaker_ids=speaker_ids)
        trp_logits = outputs["trp_logits"]  # shape [batch_size, seq_len]
        trp_probs = torch.sigmoid(trp_logits)

        # Don't append post_eos here for evaluation
        # post_eos = torch.zeros((trp_probs.size(0), 1), device=trp_probs.device)
        # trp_probs = torch.cat([trp_probs, post_eos], dim=1)

        all_probs.extend(trp_probs.cpu().numpy().flatten())
        all_labels.extend(batch["trp_labels"].cpu().numpy().flatten())


# Convert to numpy arrays
import numpy as np
all_probs = np.array(all_probs)
all_labels = np.array(all_labels)

# Compute PR AUC (threshold-free metric)
pr_auc_score = compute_pr_auc(all_labels, all_probs)
print(f"PR AUC over test set: {pr_auc_score:.4f}")

# Event-based evaluation using a threshold (try 0.5 or tune as needed)
event_metrics = evaluate_event_based(all_labels, all_probs, window=3, threshold=0.5)
print("Event-based evaluation:")
for k, v in event_metrics.items():
    print(f"{k}: {v:.4f}")

